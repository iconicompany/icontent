---
title: 'From Cosine Similarity to the "Energy" of Meanings: How Tencent's CALM Research is a Game Changer in AI Matching'
date: '2026-01-25'
description: 'This article analyzes the research from Tencent's laboratory—the CALM (Continuous Autoregressive Language Models) architecture—and its potential to transform HR-tech processes. We examine the limitations of traditional cosine similarity in skill matching and propose alternative methods: using Energy Score, creating a stable latent space through variational regularization, and increasing the semantic bandwidth of vectors. The article describes the path from "brittle" embeddings to high-precision automated talent matching systems.'
tags: ['AI', 'HRTech', 'Tencent', 'CALM', 'NLP', 'Matching']
authors: ['Iconicompany Team']
language: 'en'
---

# From Cosine Similarity to the "Energy" of Meanings: How Tencent's CALM Research is a Game Changer in AI Matching

In the world of HR-tech and automated recruitment, vector representations of skills (embeddings) have become the "gold standard." At **Iconicompany**, we are constantly seeking ways to make resume-to-vacancy matching more accurate. Traditionally, cosine similarity is used for this purpose, but is it sufficient?

A recent study by WeChat AI (Tencent) laboratory called **CALM (Continuous Autoregressive Language Models)** offers a radically new perspective on working with vectors, which could forever change our approach to matching.

### The Problem of "Brittle" Vectors

Most modern models extract skills as vectors, which are then compared in a space. However, the authors of CALM point to a critical problem: standard training methods create **"brittle representations."**

In such a space, even a small change in the wording of a skill (e.g., "Python developer" vs. "Programmer Python") can cause the vectors to be unreasonably far apart. Cosine similarity in this case yields a low match percentage, even though semantically it's the same thing.

### What CALM Teaches Us: 3 Methods for Improving Matching

Tencent's research offers several tools that we can adapt to improve the quality of matching:

#### 1. Transitioning to Energy Score

Instead of just measuring the angle between two vectors (cosine similarity), CALM uses **Energy Score**, a metric that assesses correspondence based on distances between samples.

* 
**Why it's better:** Energy Score considers not only the proximity of vectors but also their "diversity term." This allows for a more accurate assessment of not just a single skill, but an entire *set* of competencies in a resume relative to a job opening, avoiding the "collapse" of meanings.
#### 2. Creating a "Smooth" Latent Space

To make vectors robust to noise and different phrasings, the authors employ **Variational Regularization**.

* 
**How to apply it:** We can train our skill extraction models to map text not to a point in space, but to a small distribution (Gaussian posterior). Using the **KL-clipping** method (truncating KL divergence) ensures that each dimension of the vector carries useful information and doesn't turn into "white noise."



#### 3. Redundancy Through Vector Dropout

An interesting insight from the paper: using **Dropout** for vectors during training forces the model to learn redundant representations.

* This makes the matching system incredibly robust. Even if some information in a resume is presented unclearly, the model can still recover the true meaning and provide a correct match percentage.



### Our Perspective: Can the Model Be Improved?

Absolutely. CALM's approach proves that the future lies not in increasing the number of parameters, but in increasing the **"semantic bandwidth"** of each step.

For Iconicompany's tasks, this means moving from simple "word-for-word" comparison to analyzing entire "semantic chunks." CALM's autoencoder compresses a group of tokens into a single vector with a reconstruction accuracy of over 99.9%. This allows us to encode complex professional requirements into unified, dense vectors that are compared much more effectively than the arithmetic mean of individual words.

### Conclusion

Cosine similarity is a great start, but it's no longer sufficient for high-precision talent matching. Tencent's innovations in continuous models provide us with the mathematical foundation for creating more "intelligent" and robust systems. We at **Iconicompany** have already begun experimenting with the implementation of energy metrics and space regularization to help our clients find ideal candidates even faster.

*Want to learn more about how we implement cutting-edge research in practice? Subscribe to our updates!*



*Based on materials from: "Continuous Autoregressive Language Models" (Shao et al., 2025).* 

Paper: https://arxiv.org/abs/2510.27688
Code: https://github.com/shaochenze/calm

