---
title: 'From Cosine Similarity to the "Energy" of Meanings: How Tencent's CALM Research is Changing the Game in AI Matching'
date: '2026-01-25'
description: 'This article analyzes the research from Tencent's laboratory – the CALM (Continuous Autoregressive Language Models) architecture – and its potential to transform HR tech processes. We examine the limitations of traditional cosine similarity in skill matching and propose alternative methods: using Energy Score, creating a robust latent space through variational regularization, and increasing the semantic bandwidth of vectors. The article describes the journey from "brittle" embeddings to high-precision automated talent matching systems.'
tags: ['AI', 'HRTech', 'Tencent', 'CALM', 'NLP', 'Matching']
authors: ['Iconicompany Team']
language: 'en'
---

# From Cosine Similarity to the "Energy" of Meanings: How Tencent's CALM Research is Changing the Game in AI Matching

In the world of HR tech and automated recruitment, skill vector representations (embeddings) have become the "gold standard." At **Iconicompany**, we are constantly seeking ways to improve the accuracy of matching resumes and job openings. Traditionally, cosine similarity has been used for this purpose, but is it sufficient?

Recent research from WeChat AI Lab (Tencent) called **CALM (Continuous Autoregressive Language Models)** offers a radically new perspective on working with vectors, one that could forever change our approach to matching.

### The Problem of "Brittle" Vectors

Most modern models extract skills as vectors, which are then compared in a space. However, the authors of CALM point out a critical issue: standard training methods create **"brittle representations."**

In such a space, even a slight change in skill phrasing (e.g., "Python Developer" vs. "Programmer Python") can cause the vectors to be unjustifiably far apart. Cosine similarity in this case yields a low match percentage, even though semantically they are the same.

### What CALM Teaches Us: 3 Methods to Improve Matching

Tencent's research offers several tools that we can adapt to improve matching quality:

#### 1. Transition to Energy Score

Instead of simply measuring the angle between two vectors (cosine similarity), CALM uses **Energy Score**, a metric that assesses compatibility based on sample distances.

**Why it's better:** Energy Score considers not only the proximity of vectors but also their "diversity term." This allows for a more accurate assessment of an entire *set* of competencies in a resume relative to a job opening, rather than just individual skills, avoiding the "collapse" of meanings.
#### 2. Creating a "Smooth" Latent Space

To make vectors robust to noise and different phrasings, the authors employ **Variational Regularization**.

**How to apply it:** We can train our skill extraction models to map text not to a point in space, but to a small distribution (Gaussian posterior). Using the **KL-clipping** method ensures that each vector dimension carries useful information and doesn't turn into "white noise."

#### 3. Redundancy Through Vector Dropout

An interesting insight from the paper: using **Dropout** for vectors during training forces the model to learn redundant representations.

*   This makes the matching system incredibly robust. Even if some information in a resume is presented unclearly, the model can still reconstruct the true meaning and provide an accurate match percentage.

### Our Perspective: Can the Model Be Improved?

Absolutely. CALM's approach proves that the future lies not in increasing the number of parameters, but in increasing the **"semantic bandwidth"** of each step.

For Iconicompany's tasks, this means moving from simple "word-to-word" comparison to analyzing entire "semantic chunks." CALM's autoencoder compresses a group of tokens into a single vector with a reconstruction accuracy of over 99.9%. This allows us to encode complex professional requirements into unified, dense vectors that are compared much more effectively than the arithmetic mean of individual words.

### Conclusion

Cosine similarity is a great starting point, but it's no longer sufficient for high-precision talent matching. Tencent's innovations in continuous models provide us with the mathematical foundation to build smarter and more robust systems. We at **Iconicompany** have already begun experimenting with implementing energy metrics and latent space regularization to help our clients find ideal candidates even faster.

*Want to learn more about how we implement cutting-edge research into practice? Subscribe to our updates!*

*Based on the materials: "Continuous Autoregressive Language Models" (Shao et al., 2025).*

Paper: https://arxiv.org/abs/2510.27688
Code: https://github.com/shaochenze/calm

